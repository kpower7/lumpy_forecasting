{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7e88cf8",
   "metadata": {},
   "source": [
    "# Enhanced Demand Forecasting using XGBoost with Daily Data\n",
    "This script performs demand forecasting using daily demand data combined with monthly economic indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b507b4",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa45abce",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load daily data\n",
    "daily_df = pd.read_excel(\n",
    "    r\"C:\\Users\\k_pow\\OneDrive\\Documents\\MIT\\MITx SCM\\IAP 2025\\SCC\\Customer Order Quantity_Dispatched Quantity.xlsx\"\n",
    ")\n",
    "\n",
    "# Filter for Material_51\n",
    "material_id = '0001O1010'  # Material_51\n",
    "daily_df = daily_df[daily_df['Product ID'] == material_id].copy()\n",
    "\n",
    "# Convert date column\n",
    "daily_df['Date'] = pd.to_datetime(daily_df['Date'], format='%d.%m.%Y')\n",
    "daily_df.set_index('Date', inplace=True)\n",
    "daily_df.sort_index(inplace=True)\n",
    "\n",
    "# Load monthly data with external variables\n",
    "monthly_df = pd.read_csv(r'C:\\Users\\k_pow\\OneDrive\\Documents\\MIT\\MITx SCM\\IAP 2025\\SCC\\Data_files\\0001O1010_Material_51.csv')\n",
    "monthly_df['YearMonth'] = pd.to_datetime(monthly_df['YearMonth'])\n",
    "monthly_df.set_index('YearMonth', inplace=True)\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(\"Daily data shape:\", daily_df.shape)\n",
    "print(\"Monthly data shape:\", monthly_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a42763",
   "metadata": {},
   "source": [
    "## Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6531b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(daily_data, monthly_data):\n",
    "    \"\"\"Prepare features combining daily and monthly data.\"\"\"\n",
    "    features = pd.DataFrame(index=daily_data.index)\n",
    "    \n",
    "    # Extended daily demand features\n",
    "    daily_lags = [1, 2, 3, 7, 14, 30, 60, 90, 180]  \n",
    "    for lag in daily_lags:\n",
    "        features[f'Demand_lag_{lag}d'] = daily_data['Dispatched Quantity'].shift(lag)\n",
    "    \n",
    "    # Weekly aggregations - using past data only\n",
    "    weekly_demand = daily_data['Dispatched Quantity'].shift(1).resample('W').mean()\n",
    "    for lag in [1, 2, 4, 8, 12]:  \n",
    "        features[f'Demand_lag_{lag}w'] = weekly_demand.shift(lag).reindex(daily_data.index).ffill()\n",
    "    \n",
    "    # Enhanced rolling statistics on daily data\n",
    "    windows = [7, 14, 30, 60, 90]  \n",
    "    shifted_demand = daily_data['Dispatched Quantity'].shift(1)  \n",
    "    \n",
    "    for window in windows:\n",
    "        # Basic rolling statistics\n",
    "        features[f'Demand_roll_mean_{window}d'] = shifted_demand.rolling(window).mean()\n",
    "        features[f'Demand_roll_std_{window}d'] = shifted_demand.rolling(window).std()\n",
    "        \n",
    "        # Additional rolling statistics\n",
    "        features[f'Demand_roll_max_{window}d'] = shifted_demand.rolling(window).max()\n",
    "        features[f'Demand_roll_min_{window}d'] = shifted_demand.rolling(window).min()\n",
    "        \n",
    "        # Rolling quantiles\n",
    "        features[f'Demand_roll_25p_{window}d'] = shifted_demand.rolling(window).quantile(0.25)\n",
    "        features[f'Demand_roll_75p_{window}d'] = shifted_demand.rolling(window).quantile(0.75)\n",
    "    \n",
    "    # Exponential moving averages with different spans\n",
    "    for span in [7, 14, 30, 60]:\n",
    "        features[f'Demand_ema_{span}d'] = shifted_demand.ewm(span=span, adjust=False).mean()\n",
    "    \n",
    "    # Enhanced external variables features\n",
    "    external_vars = ['ECG_DESP', 'TUAV', 'PIB_CO', 'ISE_CO', 'VTOTAL_19', 'OTOTAL_19', 'ICI']\n",
    "    \n",
    "    # Create more historical lags for external variables\n",
    "    for var in external_vars:\n",
    "        # Previous months values (1-6 months lag)\n",
    "        for lag in range(1, 7):\n",
    "            features[f'{var}_lag_{lag}m'] = monthly_data[var].shift(lag).reindex(daily_data.index).ffill()\n",
    "        \n",
    "        # Rolling means of external variables (3-month and 6-month)\n",
    "        features[f'{var}_roll_mean_3m'] = monthly_data[var].rolling(3).mean().shift(1).reindex(daily_data.index).ffill()\n",
    "        features[f'{var}_roll_mean_6m'] = monthly_data[var].rolling(6).mean().shift(1).reindex(daily_data.index).ffill()\n",
    "        \n",
    "        # Rolling changes (percentage change over different periods)\n",
    "        for period in [3, 6]:\n",
    "            pct_change = monthly_data[var].pct_change(periods=period).shift(1)\n",
    "            features[f'{var}_pct_change_{period}m'] = pct_change.reindex(daily_data.index).ffill()\n",
    "    \n",
    "    # Time features (no leakage here)\n",
    "    features['DayOfWeek'] = daily_data.index.dayofweek\n",
    "    features['DayOfMonth'] = daily_data.index.day\n",
    "    features['WeekOfYear'] = daily_data.index.isocalendar().week\n",
    "    features['Month'] = daily_data.index.month\n",
    "    features['Year'] = daily_data.index.year - 2022\n",
    "    \n",
    "    # Enhanced customer order features\n",
    "    for lag in [1, 2, 3, 7, 14, 30]:  \n",
    "        features[f'Customer_Order_Lag{lag}'] = daily_data['Customer Order Quantity'].shift(lag)\n",
    "        # Order-Demand ratio with multiple lags\n",
    "        ratio = (daily_data['Customer Order Quantity'].shift(lag) / \n",
    "                daily_data['Dispatched Quantity'].shift(lag))\n",
    "        features[f'Order_Demand_Ratio_Lag{lag}'] = ratio.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    \n",
    "    # Rolling means for customer orders\n",
    "    shifted_orders = daily_data['Customer Order Quantity'].shift(1)\n",
    "    for window in [7, 14, 30]:\n",
    "        features[f'Customer_Order_Roll_Mean_{window}d'] = shifted_orders.rolling(window).mean()\n",
    "    \n",
    "    return features.fillna(0)\n",
    "\n",
    "# Prepare features\n",
    "features_df = prepare_features(daily_df, monthly_df)\n",
    "target = daily_df['Dispatched Quantity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c6dfe0",
   "metadata": {},
   "source": [
    "## Split Data and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fa2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data based on date\n",
    "train_cutoff = pd.to_datetime('2024-06-01')\n",
    "train_mask = features_df.index < train_cutoff\n",
    "\n",
    "X_train = features_df[train_mask]\n",
    "y_train = target[train_mask]\n",
    "X_test = features_df[~train_mask]\n",
    "y_test = target[~train_mask]\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    min_child_weight=2,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"Model training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045f686",
   "metadata": {},
   "source": [
    "## Evaluate Model at Multiple Time Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ee88e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Make daily predictions\n",
    "daily_pred = model.predict(X_test)\n",
    "\n",
    "# Create DataFrames with actual and predicted values at different scales\n",
    "daily_results = pd.DataFrame({\n",
    "    'Date': X_test.index,\n",
    "    'Actual': y_test,\n",
    "    'Predicted': daily_pred\n",
    "})\n",
    "\n",
    "# Aggregate to weekly\n",
    "weekly_actual = y_test.resample('W-MON').mean()\n",
    "weekly_pred = pd.Series(daily_pred, index=X_test.index).resample('W-MON').mean()\n",
    "weekly_results = pd.DataFrame({\n",
    "    'Date': weekly_actual.index,\n",
    "    'Actual': weekly_actual,\n",
    "    'Predicted': weekly_pred\n",
    "})\n",
    "\n",
    "# Aggregate to monthly\n",
    "monthly_actual = y_test.resample('MS').mean()\n",
    "monthly_pred = pd.Series(daily_pred, index=X_test.index).resample('MS').mean()\n",
    "monthly_results = pd.DataFrame({\n",
    "    'Date': monthly_actual.index,\n",
    "    'Actual': monthly_actual,\n",
    "    'Predicted': monthly_pred\n",
    "})\n",
    "\n",
    "# Calculate metrics for each time scale\n",
    "def calculate_metrics(actual, pred):\n",
    "    return {\n",
    "        'RMSE': np.sqrt(mean_squared_error(actual, pred)),\n",
    "        'MAE': mean_absolute_error(actual, pred),\n",
    "        'R2': r2_score(actual, pred)\n",
    "    }\n",
    "\n",
    "daily_metrics = calculate_metrics(daily_results['Actual'], daily_results['Predicted'])\n",
    "weekly_metrics = calculate_metrics(weekly_results['Actual'], weekly_results['Predicted'])\n",
    "monthly_metrics = calculate_metrics(monthly_results['Actual'], monthly_results['Predicted'])\n",
    "\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "print(\"\\nDaily Metrics:\")\n",
    "print(f\"RMSE: {daily_metrics['RMSE']:.2f}\")\n",
    "print(f\"MAE: {daily_metrics['MAE']:.2f}\")\n",
    "print(f\"R2: {daily_metrics['R2']:.3f}\")\n",
    "\n",
    "print(\"\\nWeekly Metrics:\")\n",
    "print(f\"RMSE: {weekly_metrics['RMSE']:.2f}\")\n",
    "print(f\"MAE: {weekly_metrics['MAE']:.2f}\")\n",
    "print(f\"R2: {weekly_metrics['R2']:.3f}\")\n",
    "\n",
    "print(\"\\nMonthly Metrics:\")\n",
    "print(f\"RMSE: {monthly_metrics['RMSE']:.2f}\")\n",
    "print(f\"MAE: {monthly_metrics['MAE']:.2f}\")\n",
    "print(f\"R2: {monthly_metrics['R2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6ab99c",
   "metadata": {},
   "source": [
    "## Visualize Results at Different Time Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190f32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(results, title, freq):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results['Date'], results['Actual'], label='Actual', marker='o')\n",
    "    plt.plot(results['Date'], results['Predicted'], label='Predicted', marker='s')\n",
    "    plt.title(f'Material 51: Actual vs Predicted Demand ({freq})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results at different time scales\n",
    "plot_predictions(daily_results, 'Daily Predictions', 'Daily')\n",
    "plot_predictions(weekly_results, 'Weekly Predictions', 'Weekly')\n",
    "plot_predictions(monthly_results, 'Monthly Predictions', 'Monthly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f61698",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(importance.head(20)['feature'], importance.head(20)['importance'])\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance.head(10))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
